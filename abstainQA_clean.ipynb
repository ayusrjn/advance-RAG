{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AbstainQA  Confidence-Gated RAG System\n",
        "\n",
        "Designed a retrieval-based QA system that deterministically abstains when document evidence is insufficient. Implemented confidence estimation from embedding similarity, enforced LLM grounding via system constraints, and validated safe failure behavior under empty and low-relevance retrieval."
      ],
      "metadata": {
        "id": "vP0Q0VaClQVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken\n",
        "!pip install -q langchain-huggingface sentence-transformers"
      ],
      "metadata": {
        "id": "2tSKa4mrKfcA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "DHa6H0aYcaE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "u3lZdF8faC8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fvqtpCCdKRQ7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Generator\n",
        "import tiktoken\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    \"\"\"Represents a single document chunk with metadata.\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    source_file: str\n",
        "    section_path: list[str]\n",
        "    token_count: int\n",
        "    has_table: bool\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return asdict(self)\n",
        "\n",
        "class MarkdownChunker:\n",
        "    \"\"\"Chunks markdown documents based on structural elements.\"\"\"\n",
        "    HEADER_PATTERN = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n",
        "    TABLE_PATTERN = re.compile(r'^\\|.+\\|$\\n^\\|[-:\\s|]+\\|$(?:\\n^\\|.+\\|$)+', re.MULTILINE)\n",
        "\n",
        "    def __init__(self, max_tokens: int = 1000, model: str = \"cl100k_base\"):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.tokenizer = tiktoken.get_encoding(model)\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(self.tokenizer.encode(text))\n",
        "\n",
        "    def generate_chunk_id(self, source: str, content: str, index: int) -> str:\n",
        "        hash_input = f\"{source}:{index}:{content[:100]}\"\n",
        "        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]\n",
        "\n",
        "    def extract_tables(self, text: str) -> list[tuple[int, int, str]]:\n",
        "        tables = []\n",
        "        for match in self.TABLE_PATTERN.finditer(text):\n",
        "            tables.append((match.start(), match.end(), match.group()))\n",
        "        return tables\n",
        "\n",
        "    def parse_sections(self, content: str) -> list[dict]:\n",
        "        lines = content.split('\\n')\n",
        "        sections = []\n",
        "        current_section = {'level': 0, 'title': '', 'content_lines': [], 'has_table': False}\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            line = lines[i]\n",
        "            header_match = self.HEADER_PATTERN.match(line)\n",
        "            if header_match:\n",
        "                if current_section['content_lines'] or current_section['title']:\n",
        "                    section_content = '\\n'.join(current_section['content_lines'])\n",
        "                    current_section['has_table'] = bool(self.TABLE_PATTERN.search(section_content))\n",
        "                    sections.append({\n",
        "                        'level': current_section['level'],\n",
        "                        'title': current_section['title'],\n",
        "                        'content': section_content.strip(),\n",
        "                        'has_table': current_section['has_table']\n",
        "                    })\n",
        "                level = len(header_match.group(1))\n",
        "                title = header_match.group(2).strip()\n",
        "                current_section = {'level': level, 'title': title, 'content_lines': [], 'has_table': False}\n",
        "            else:\n",
        "                current_section['content_lines'].append(line)\n",
        "            i += 1\n",
        "        if current_section['content_lines'] or current_section['title']:\n",
        "            section_content = '\\n'.join(current_section['content_lines'])\n",
        "            current_section['has_table'] = bool(self.TABLE_PATTERN.search(section_content))\n",
        "            sections.append({\n",
        "                'level': current_section['level'],\n",
        "                'title': current_section['title'],\n",
        "                'content': section_content.strip(),\n",
        "                'has_table': current_section['has_table']\n",
        "            })\n",
        "        return sections\n",
        "\n",
        "    def build_section_path(self, sections: list[dict], current_idx: int) -> list[str]:\n",
        "        current = sections[current_idx]\n",
        "        path = []\n",
        "        if current['title']: path.append(current['title'])\n",
        "        current_level = current['level']\n",
        "        for i in range(current_idx - 1, -1, -1):\n",
        "            section = sections[i]\n",
        "            if section['level'] < current_level and section['title']:\n",
        "                path.insert(0, section['title'])\n",
        "                current_level = section['level']\n",
        "                if current_level == 1: break\n",
        "        return path\n",
        "\n",
        "    def _build_header_prefix(self, section_path: list[str]) -> str:\n",
        "        if not section_path: return \"\"\n",
        "        lines = []\n",
        "        for i, title in enumerate(section_path):\n",
        "            lines.append('#' * (i + 1) + ' ' + title)\n",
        "        return '\\n'.join(lines) + '\\n\\n'\n",
        "\n",
        "    def split_large_section(self, text: str, section_path: list[str], source_file: str, start_index: int) -> Generator[Chunk, None, None]:\n",
        "        # (Included from your snippet)\n",
        "        header_prefix = self._build_header_prefix(section_path)\n",
        "        header_tokens = self.count_tokens(header_prefix)\n",
        "        available_tokens = self.max_tokens - header_tokens\n",
        "        paragraphs = re.split(r'\\n\\n+', text)\n",
        "        current_chunk_parts = []\n",
        "        current_tokens = 0\n",
        "        chunk_index = start_index\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if not para: continue\n",
        "            para_tokens = self.count_tokens(para)\n",
        "            is_table = bool(self.TABLE_PATTERN.search(para))\n",
        "\n",
        "            if is_table and para_tokens > available_tokens:\n",
        "                if current_chunk_parts:\n",
        "                    content = header_prefix + '\\n\\n'.join(current_chunk_parts)\n",
        "                    yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), False)\n",
        "                    chunk_index += 1; current_chunk_parts = []; current_tokens = 0\n",
        "                content = header_prefix + para\n",
        "                yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), True)\n",
        "                chunk_index += 1\n",
        "                continue\n",
        "\n",
        "            if current_tokens + para_tokens > available_tokens:\n",
        "                if current_chunk_parts:\n",
        "                    chunk_text = '\\n\\n'.join(current_chunk_parts)\n",
        "                    content = header_prefix + chunk_text\n",
        "                    yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), bool(self.TABLE_PATTERN.search(chunk_text)))\n",
        "                    chunk_index += 1; current_chunk_parts = []; current_tokens = 0\n",
        "\n",
        "            current_chunk_parts.append(para)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "        if current_chunk_parts:\n",
        "            chunk_text = '\\n\\n'.join(current_chunk_parts)\n",
        "            content = header_prefix + chunk_text\n",
        "            yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), bool(self.TABLE_PATTERN.search(chunk_text)))\n",
        "\n",
        "    def chunk_document(self, content: str, source_file: str) -> Generator[Chunk, None, None]:\n",
        "        sections = self.parse_sections(content)\n",
        "        chunk_index = 0\n",
        "        for i, section in enumerate(sections):\n",
        "            section_path = self.build_section_path(sections, i)\n",
        "            header = ('#' * section['level'] + ' ' + section['title']) if section['title'] else ''\n",
        "            full_text = (header + '\\n\\n' + section['content']).strip() if header else section['content'].strip()\n",
        "            if not full_text: continue\n",
        "\n",
        "            token_count = self.count_tokens(full_text)\n",
        "            if token_count <= self.max_tokens:\n",
        "                yield Chunk(self.generate_chunk_id(source_file, full_text, chunk_index), full_text, source_file, section_path, token_count, section['has_table'])\n",
        "                chunk_index += 1\n",
        "            else:\n",
        "                for chunk in self.split_large_section(section['content'], section_path, source_file, chunk_index):\n",
        "                    yield chunk\n",
        "                    chunk_index += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "def get_embedding_model():\n",
        "    \"\"\"\n",
        "    Initializes the BGE-M3 embedding model.\n",
        "    \"\"\"\n",
        "    # Check for GPU availability in Colab\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading BGE-M3 model on: {device}\")\n",
        "\n",
        "    # Configuration for BGE-M3\n",
        "    model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "    # Model keyword arguments\n",
        "    model_kwargs = {\n",
        "        \"device\": device,\n",
        "        # Trust remote code is sometimes needed for specific architectures,\n",
        "        # but usually safe for standard BGE. Keeps it robust.\n",
        "        \"trust_remote_code\": True\n",
        "    }\n",
        "\n",
        "    # Encoding keyword arguments\n",
        "    # Normalize embeddings is crucial for Cosine Similarity\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }\n",
        "\n",
        "    # Initialize the LangChain wrapper\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "\n",
        "    return embedding_model"
      ],
      "metadata": {
        "id": "Lfl2TGUiKpkF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GuardrailManager:\n",
        "    def __init__(self, confidence_threshold=0.35): # Slightly lowered threshold for robustness\n",
        "        self.CONFIDENCE_THRESHOLD = confidence_threshold\n",
        "\n",
        "    def calculate_confidence(self, retrieved_chunks, k=3):\n",
        "        if not retrieved_chunks:\n",
        "            return 0.0\n",
        "\n",
        "        distances = np.array([c[\"distance\"] for c in retrieved_chunks])\n",
        "        sims = 1 / (1 + distances)\n",
        "\n",
        "        sims_sorted = np.sort(sims)[::-1]\n",
        "        top1 = sims_sorted[0]\n",
        "        topk = sims_sorted[:k]\n",
        "        avg_topk = topk.mean()\n",
        "\n",
        "        gap = top1 - (sims_sorted[1] if len(sims_sorted) > 1 else 0.0)\n",
        "\n",
        "        # Evidence aggregation with safety anchor\n",
        "        confidence = (\n",
        "            0.6 * top1          # sufficiency\n",
        "          + 0.25 * avg_topk     # consistency\n",
        "          + 0.15 * gap          # ambiguity penalty\n",
        "        )\n",
        "\n",
        "        return round(float(min(confidence, 1.0)), 2)\n",
        "\n",
        "\n",
        "\n",
        "    def check_guardrails(self, query, retrieved_chunks, confidence_score):\n",
        "        \"\"\"\n",
        "        Applies strict failure logic.\n",
        "        \"\"\"\n",
        "        # Case 1: Empty Retrieval\n",
        "        if not retrieved_chunks:\n",
        "            return False, \"Abstain: No relevant documents found.\"\n",
        "\n",
        "        # Case 2: Confidence Threshold\n",
        "        if confidence_score < self.CONFIDENCE_THRESHOLD:\n",
        "            return False, f\"Abstain: Confidence score {confidence_score} is below threshold {self.CONFIDENCE_THRESHOLD}.\"\n",
        "\n",
        "        return True, \"Passed\"\n",
        "\n"
      ],
      "metadata": {
        "id": "crJMpnuwKslz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb import EmbeddingFunction, Documents, Embeddings\n",
        "import uuid\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import glob\n",
        "from langchain_core.documents import Document  # The correct new import\n",
        "\n",
        "# --- 1. UPDATED VECTOR STORE MANAGER ---\n",
        "class ChromaEmbeddingAdapter(EmbeddingFunction):\n",
        "    def __init__(self, langchain_embeddings):\n",
        "        self.ef = langchain_embeddings\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        return self.ef.embed_documents(input)\n",
        "    def name(self):\n",
        "        return \"bge-m3\"\n",
        "\n",
        "class VectorStoreManager:\n",
        "    def __init__(self, embedding_function, collection_name=\"uoh_policies\", persist_path=\"./policy_db_v3\"):\n",
        "        \"\"\"\n",
        "        UPDATED: Now accepts 'persist_path' to fix the error.\n",
        "        \"\"\"\n",
        "        self.client = chromadb.PersistentClient(path=persist_path)\n",
        "        self.embedding_adapter = ChromaEmbeddingAdapter(embedding_function)\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=collection_name,\n",
        "            embedding_function=self.embedding_adapter\n",
        "        )\n",
        "        print(f\"Collection '{collection_name}' loaded from '{persist_path}'.\")\n",
        "\n",
        "    def add_chunks(self, chunks):\n",
        "        if not chunks: return\n",
        "        ids = [str(uuid.uuid4()) for _ in chunks]\n",
        "        documents = [chunk.page_content for chunk in chunks]\n",
        "        metadatas = [chunk.metadata for chunk in chunks]\n",
        "        self.collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
        "        print(f\"Added {len(chunks)} chunks.\")\n",
        "\n",
        "    def retrieve_context(self, query, top_k=5):\n",
        "        results = self.collection.query(\n",
        "            query_texts=[query], n_results=top_k, include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "        )\n",
        "        retrieved_chunks = []\n",
        "        if results['ids'] and results['ids'][0]:\n",
        "            for i in range(len(results['ids'][0])):\n",
        "                retrieved_chunks.append({\n",
        "                    \"id\": results['ids'][0][i],\n",
        "                    \"text\": results['documents'][0][i],\n",
        "                    \"metadata\": results['metadatas'][0][i],\n",
        "                    \"distance\": results['distances'][0][i]\n",
        "                })\n",
        "        return retrieved_chunks\n",
        "\n",
        "# --- 2. UPDATED RAG PIPELINE ---\n",
        "class RAGPipeline:\n",
        "    def __init__(self, db_path=\"./policy_db_v3\"):\n",
        "        print(\"Initializing RAG Pipeline with Custom MarkdownChunker...\")\n",
        "\n",
        "        # Initialize your custom chunker\n",
        "        self.chunker = MarkdownChunker(max_tokens=1000)\n",
        "\n",
        "        self.embedding_model = get_embedding_model()\n",
        "\n",
        "        # Now VectorStoreManager will accept the 'persist_path' argument\n",
        "        self.vector_db = VectorStoreManager(embedding_function=self.embedding_model, persist_path=db_path)\n",
        "        self.guardrails = GuardrailManager()\n",
        "        self.llm = LLMGenerator()\n",
        "        print(\"Pipeline Ready.\")\n",
        "\n",
        "    def ingest_document(self, markdown_text: str, source_name: str):\n",
        "        # 1. Generate custom chunks\n",
        "        custom_chunks = list(self.chunker.chunk_document(markdown_text, source_name))\n",
        "\n",
        "        # 2. Convert to LangChain Documents for Chroma\n",
        "        compatible_chunks = []\n",
        "        for c in custom_chunks:\n",
        "            meta = {\n",
        "                \"id\": c.id,\n",
        "                \"source\": c.source_file,\n",
        "                \"has_table\": str(c.has_table),\n",
        "                \"token_count\": c.token_count,\n",
        "                \"section_path\": \" > \".join(c.section_path)\n",
        "            }\n",
        "            doc = Document(page_content=c.content, metadata=meta)\n",
        "            compatible_chunks.append(doc)\n",
        "\n",
        "        print(f\"Adding {len(compatible_chunks)} chunks from '{source_name}'...\")\n",
        "        self.vector_db.add_chunks(compatible_chunks)\n",
        "\n",
        "    def query(self, user_query: str):\n",
        "        print(f\"\\nProcessing Query: '{user_query}'\")\n",
        "        retrieved_chunks = self.vector_db.retrieve_context(user_query)\n",
        "        retrieval_confidence = self.guardrails.calculate_confidence(retrieved_chunks)\n",
        "        passed, reason = self.guardrails.check_guardrails(user_query, retrieved_chunks, retrieval_confidence)\n",
        "\n",
        "        if not passed:\n",
        "            return {\n",
        "                \"answer\": \"I cannot answer this question based on the provided policy documents.\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        raw_response = self.llm.generate(user_query, retrieved_chunks)\n",
        "        try:\n",
        "            final_json = json.loads(raw_response)\n",
        "            final_json['confidence'] = min(final_json.get('confidence', 0.0), retrieval_confidence)\n",
        "            return final_json\n",
        "        except json.JSONDecodeError:\n",
        "             return {\"answer\": \"Error parsing model response.\", \"confidence\": 0.0, \"sources\": []}\n",
        "\n"
      ],
      "metadata": {
        "id": "9VMrmgUyZu-M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMGenerator:\n",
        "    def __init__(self, model_name=\"gemini-1.5-flash-001\"): # Updated to canonical ID\n",
        "        # 1. Configure API (Ensure key is set)\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDE94dIBM4Qm2BD9jzI4wO1nyI-AOvKVQs\"\n",
        "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        # 2. List models to verify availability (Safety check)\n",
        "        try:\n",
        "            self.model = genai.GenerativeModel(\n",
        "                model_name=model_name,\n",
        "                generation_config={\"response_mime_type\": \"application/json\"}\n",
        "            )\n",
        "            print(f\"✅ LLM initialized with model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error init model '{model_name}': {e}\")\n",
        "            print(\"Falling back to 'gemini-pro'...\")\n",
        "            self.model = genai.GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "    def generate(self, query: str, context_chunks: list) -> str:\n",
        "        # Format context\n",
        "        context_str = \"\"\n",
        "        for i, chunk in enumerate(context_chunks):\n",
        "            # Handle potential missing keys safely\n",
        "            text = chunk.get('text', '')\n",
        "            cid = chunk.get('id', 'unknown')\n",
        "            context_str += f\"Source {i+1} (ID: {cid}):\\n{text}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a strict policy assistant. Answer based ONLY on the sources.\n",
        "\n",
        "        ## USER QUERY:\n",
        "        \"{query}\"\n",
        "\n",
        "        ## SOURCES:\n",
        "        {context_str}\n",
        "\n",
        "        ## INSTRUCTIONS:\n",
        "        1. Answer strictly using the sources.\n",
        "        2. If the info is missing, return \"confidence\": 0.0.\n",
        "        3. Output VALID JSON.\n",
        "\n",
        "        ## OUTPUT JSON:\n",
        "        {{\n",
        "            \"answer\": \"...\",\n",
        "            \"confidence\": <float>,\n",
        "            \"sources\": [\"source_id\"]\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return json.dumps({\n",
        "                \"answer\": f\"LLM Generation Error: {str(e)}\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"sources\": []\n",
        "            })"
      ],
      "metadata": {
        "id": "nc-c1Q8PaAWZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_system = RAGPipeline()\n",
        "\n"
      ],
      "metadata": {
        "id": "YIV6DHfAaEsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "md_files = glob.glob(\"./policies/*.md\")\n",
        "if md_files:\n",
        "    print(f\"Re-ingesting {len(md_files)} files...\")\n",
        "    for file_path in md_files:\n",
        "        try:\n",
        "            filename = os.path.basename(file_path)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                rag_system.ingest_document(f.read(), source_name=filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {file_path}: {e}\")\n",
        "    print(\"✅ Database rebuilt with Cosine Similarity!\")\n",
        "else:\n",
        "    print(\"❌ No files found in ./policies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA4jZjjVag4j",
        "outputId": "f2514898-9cad-4c64-b08b-69d88ae8d8db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-ingesting 11 files...\n",
            "Adding 11 chunks from 'Consultancy-Policy-_-Guidelines_compressed.md'...\n",
            "Added 11 chunks.\n",
            "Adding 37 chunks from 'University-of-Hyderabad-Act_compressed.md'...\n",
            "Added 37 chunks.\n",
            "Adding 5 chunks from 'Policy-for-Student-Assistanceship-and-Scholarship_compressed.md'...\n",
            "Added 5 chunks.\n",
            "Adding 21 chunks from 'Information-Technology-Policy_compressed.md'...\n",
            "Added 21 chunks.\n",
            "Adding 10 chunks from 'Reservation-Policy-Student-Admissions-_compressed.md'...\n",
            "Added 10 chunks.\n",
            "Adding 13 chunks from 'POLICY.md'...\n",
            "Added 13 chunks.\n",
            "Adding 3 chunks from 'Reservation-Policy-for-Appointment-of-Teachers-and-Staff_compressed.md'...\n",
            "Added 3 chunks.\n",
            "Adding 22 chunks from 'admission_policy.md'...\n",
            "Added 22 chunks.\n",
            "Adding 18 chunks from 'Research-Policy-_compressed.md'...\n",
            "Added 18 chunks.\n",
            "Adding 9 chunks from 'ForeignStudentAdmissionPolicy_compressed.md'...\n",
            "Added 9 chunks.\n",
            "Adding 19 chunks from 'Teaching-and-Evaluation-Regulations_compressed.md'...\n",
            "Added 19 chunks.\n",
            "✅ Database rebuilt with Cosine Similarity!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_system.llm = LLMGenerator(model_name=\"gemini-3-flash-preview\")\n",
        "rag_system.guardrails = GuardrailManager(confidence_threshold=0.35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgrrDRf-bBGJ",
        "outputId": "cbb46f95-cc6a-408d-fed7-4378bf6ecd9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM initialized with model: gemini-3-flash-preview\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 40)\n",
        "response = rag_system.query(\"How to secure myself\")\n",
        "print(json.dumps(response, indent=2))\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "X_hd9KhUdfFW",
        "outputId": "03002804-db86-4849-e46e-f71f335a11ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "\n",
            "Processing Query: 'How to secure myself'\n",
            "{\n",
            "  \"answer\": \"According to the university IT policies, you can secure yourself and your systems by following these guidelines:\\n\\n1. **System Responsibility**: You are responsible for the security and integrity of your own systems. If a computer is 'hacked into,' it is recommended that the system be shut down immediately to limit potential damage and prevent the attack from spreading. You must take reasonable steps to ensure the machine is not compromised before network privileges are restored (Source 2).\\n2. **Data Protection**: Perform backups of critical data in compliance with the requirements of the IT Act of India (Source 2).\\n3. **Identity and Communication**: Ensure all electronic communications accurately identify you as the sender; the use of anonymous or masquerading mail forwarders is prohibited (Source 2, 5). Do not use official university email addresses to register for personal social networks or blogs (Source 4).\\n4. **Credential and Access Security**: Never use another person's ID without explicit permission and ensure your assigned ID is not used by others (Source 5). Do not use loopholes or knowledge of special passwords to alter systems or take resources from others (Source 3).\\n5. **Privacy**: Treat all files and network traffic as private and confidential unless the owner has explicitly made them available (Source 5).\\n6. **Social Media and Intellectual Property**: Avoid sharing intellectual property like trademarks on personal accounts and ensure personal statements include a disclaimer that they do not represent the University (Source 4).\",\n",
            "  \"confidence\": 0.42,\n",
            "  \"sources\": [\n",
            "    \"31d54073-6b79-440e-8ae5-082d5dd2f994\",\n",
            "    \"654d447a-5d8c-4b01-b37c-b5754ea7ebde\",\n",
            "    \"a12ba587-8f1e-4f72-9059-f6bae914b42b\",\n",
            "    \"fff7c134-5818-4b21-8e67-27d47b926e0d\"\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j81vasK0knxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}