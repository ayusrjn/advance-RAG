{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXx8OOZFoRa-"
      },
      "outputs": [],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# 1. Setup paths\n",
        "input_dir = Path(\"/content/reports\")\n",
        "output_dir = Path(\"extracted_md\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# 2. Initialize Docling Converter\n",
        "converter = DocumentConverter()\n",
        "\n",
        "def extract_pdfs():\n",
        "    pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {input_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} files. Starting extraction...\")\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            print(f\"Processing: {pdf_path.name}...\")\n",
        "\n",
        "            # Convert PDF to Docling's internal format\n",
        "            result = converter.convert(pdf_path)\n",
        "\n",
        "            # Export to Markdown (best for your chunking strategy)\n",
        "            md_output = result.document.export_to_markdown()\n",
        "\n",
        "            # Save the file\n",
        "            output_filename = pdf_path.stem + \".md\"\n",
        "            with open(output_dir / output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(md_output)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "    print(f\"\\nSuccess! Extracted files are in: {output_dir}\")\n",
        "\n",
        "extract_pdfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5EaUZQee4Pe",
        "outputId": "7a375f84-be94-42a8-b436-e388e70a671f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 files. Starting extraction...\n",
            "Processing: Bihar_Budget_Analysis_2025-26.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2026-02-02 13:13:29,853 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:29,858 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:29,861 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.6.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:30,799 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:30,936 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:30,938 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:31,558 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:31,559 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:31,561 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.6.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,553 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,567 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,569 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,653 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,654 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:32,655 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.6.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:33,914 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:34,160 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-02-02 13:13:34,161 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: State_Budget_Analysis_2021-22-Bihar.pdf...\n",
            "Processing: Bihar Budget Analysis 2022-23.pdf...\n",
            "Processing: State Budget Analysis - Bihar 2019-20 English.pdf...\n",
            "Processing: Bihar_Budget_Analysis_2024-25.pdf...\n",
            "Processing: Bihar Budget Analysis - 2018-19.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:docling.pipeline.standard_pdf_pipeline:Stage preprocess failed for run 7, pages [2]: Failed to load page.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Bihar Budget Analysis 2016-17.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:docling.pipeline.standard_pdf_pipeline:Stage preprocess failed for run 7, pages [5]: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Bihar_Budget_Analysis_2023-24.pdf...\n",
            "Processing: State Budget Analysis - Bihar 2020-21.pdf...\n",
            "Processing: Bihar Budget Analysis 2017-18.pdf...\n",
            "\n",
            "Success! Extracted files are in: extracted_md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rPohTNxjWX9",
        "outputId": "43c1e26a-a10a-4c51-f6bb-683bfe51f6fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "import yaml\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from pathlib import Path\n",
        "from typing import Generator, Optional\n",
        "import tiktoken\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    \"\"\"Represents a single document chunk with metadata.\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    source_file: str\n",
        "    section_path: list[str]\n",
        "    token_count: int\n",
        "    has_table: bool\n",
        "    # New field: Inherits doc_id, version, date, etc.\n",
        "    metadata: dict = field(default_factory=dict)\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return asdict(self)\n",
        "\n",
        "class MarkdownChunker:\n",
        "    \"\"\"Chunks markdown documents based on structural elements and preserves metadata.\"\"\"\n",
        "\n",
        "    # Pattern to separate YAML frontmatter from Markdown body\n",
        "    FRONTMATTER_PATTERN = re.compile(r'^---\\s*\\n(.*?)\\n---\\s*\\n', re.DOTALL)\n",
        "    HEADER_PATTERN = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n",
        "    TABLE_PATTERN = re.compile(r'^\\|.+\\|(?:\\n^\\|.+\\|$)+', re.MULTILINE)\n",
        "\n",
        "    def __init__(self, max_tokens: int = 1000, model: str = \"cl100k_base\"):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.tokenizer = tiktoken.get_encoding(model)\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(self.tokenizer.encode(text))\n",
        "\n",
        "    def generate_chunk_id(self, source: str, content: str, index: int) -> str:\n",
        "        # Including index ensures unique IDs even for identical content\n",
        "        hash_input = f\"{source}:{index}:{content[:100]}\"\n",
        "        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]\n",
        "\n",
        "    def _parse_frontmatter(self, text: str) -> tuple[dict, str]:\n",
        "        \"\"\"Extracts YAML frontmatter and returns (metadata_dict, remaining_content).\"\"\"\n",
        "        match = self.FRONTMATTER_PATTERN.match(text)\n",
        "        if match:\n",
        "            yaml_content = match.group(1)\n",
        "            try:\n",
        "                metadata = yaml.safe_load(yaml_content)\n",
        "                # Return metadata and the text strictly AFTER the --- block\n",
        "                return metadata, text[match.end():]\n",
        "            except yaml.YAMLError:\n",
        "                # Fallback if YAML is malformed\n",
        "                return {}, text\n",
        "        return {}, text\n",
        "\n",
        "    def parse_sections(self, content: str) -> list[dict]:\n",
        "        lines = content.split('\\n')\n",
        "        sections = []\n",
        "        current_section = {'level': 0, 'title': '', 'content_lines': [], 'has_table': False}\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            line = lines[i]\n",
        "            header_match = self.HEADER_PATTERN.match(line)\n",
        "            if header_match:\n",
        "                # Save previous section\n",
        "                if current_section['content_lines'] or current_section['title']:\n",
        "                    section_content = '\\n'.join(current_section['content_lines'])\n",
        "                    current_section['has_table'] = bool(self.TABLE_PATTERN.search(section_content))\n",
        "                    sections.append({\n",
        "                        'level': current_section['level'],\n",
        "                        'title': current_section['title'],\n",
        "                        'content': section_content.strip(),\n",
        "                        'has_table': current_section['has_table']\n",
        "                    })\n",
        "                # Start new section\n",
        "                level = len(header_match.group(1))\n",
        "                title = header_match.group(2).strip()\n",
        "                current_section = {'level': level, 'title': title, 'content_lines': [], 'has_table': False}\n",
        "            else:\n",
        "                current_section['content_lines'].append(line)\n",
        "            i += 1\n",
        "\n",
        "        # Capture the last section\n",
        "        if current_section['content_lines'] or current_section['title']:\n",
        "            section_content = '\\n'.join(current_section['content_lines'])\n",
        "            current_section['has_table'] = bool(self.TABLE_PATTERN.search(section_content))\n",
        "            sections.append({\n",
        "                'level': current_section['level'],\n",
        "                'title': current_section['title'],\n",
        "                'content': section_content.strip(),\n",
        "                'has_table': current_section['has_table']\n",
        "            })\n",
        "        return sections\n",
        "\n",
        "    def build_section_path(self, sections: list[dict], current_idx: int) -> list[str]:\n",
        "        current = sections[current_idx]\n",
        "        path = []\n",
        "        if current['title']: path.append(current['title'])\n",
        "        current_level = current['level']\n",
        "        for i in range(current_idx - 1, -1, -1):\n",
        "            section = sections[i]\n",
        "            if section['level'] < current_level and section['title']:\n",
        "                path.insert(0, section['title'])\n",
        "                current_level = section['level']\n",
        "                if current_level == 1: break\n",
        "        return path\n",
        "\n",
        "    def _build_header_prefix(self, section_path: list[str]) -> str:\n",
        "        if not section_path: return \"\"\n",
        "        lines = []\n",
        "        for i, title in enumerate(section_path):\n",
        "            lines.append('#' * (i + 1) + ' ' + title)\n",
        "        return '\\n'.join(lines) + '\\n\\n'\n",
        "\n",
        "    def split_large_section(self, text: str, section_path: list[str], source_file: str, start_index: int, doc_metadata: dict) -> Generator[Chunk, None, None]:\n",
        "        header_prefix = self._build_header_prefix(section_path)\n",
        "        header_tokens = self.count_tokens(header_prefix)\n",
        "        available_tokens = self.max_tokens - header_tokens\n",
        "        paragraphs = re.split(r'\\n\\n+', text)\n",
        "        current_chunk_parts = []\n",
        "        current_tokens = 0\n",
        "        chunk_index = start_index\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if not para: continue\n",
        "            para_tokens = self.count_tokens(para)\n",
        "            is_table = bool(self.TABLE_PATTERN.search(para))\n",
        "\n",
        "            # Case 1: Table is too big or forces a break\n",
        "            if is_table and para_tokens > available_tokens:\n",
        "                if current_chunk_parts:\n",
        "                    content = header_prefix + '\\n\\n'.join(current_chunk_parts)\n",
        "                    yield Chunk(\n",
        "                        self.generate_chunk_id(source_file, content, chunk_index),\n",
        "                        content, source_file, section_path, self.count_tokens(content), False,\n",
        "                        doc_metadata\n",
        "                    )\n",
        "                    chunk_index += 1; current_chunk_parts = []; current_tokens = 0\n",
        "\n",
        "                content = header_prefix + para\n",
        "                yield Chunk(\n",
        "                    self.generate_chunk_id(source_file, content, chunk_index),\n",
        "                    content, source_file, section_path, self.count_tokens(content), True,\n",
        "                    doc_metadata\n",
        "                )\n",
        "                chunk_index += 1\n",
        "                continue\n",
        "\n",
        "            # Case 2: Paragraph overflows current chunk\n",
        "            if current_tokens + para_tokens > available_tokens:\n",
        "                if current_chunk_parts:\n",
        "                    chunk_text = '\\n\\n'.join(current_chunk_parts)\n",
        "                    content = header_prefix + chunk_text\n",
        "                    yield Chunk(\n",
        "                        self.generate_chunk_id(source_file, content, chunk_index),\n",
        "                        content, source_file, section_path, self.count_tokens(content),\n",
        "                        bool(self.TABLE_PATTERN.search(chunk_text)),\n",
        "                        doc_metadata\n",
        "                    )\n",
        "                    chunk_index += 1; current_chunk_parts = []; current_tokens = 0\n",
        "\n",
        "            current_chunk_parts.append(para)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "        # Yield remaining parts\n",
        "        if current_chunk_parts:\n",
        "            chunk_text = '\\n\\n'.join(current_chunk_parts)\n",
        "            content = header_prefix + chunk_text\n",
        "            yield Chunk(\n",
        "                self.generate_chunk_id(source_file, content, chunk_index),\n",
        "                content, source_file, section_path, self.count_tokens(content),\n",
        "                bool(self.TABLE_PATTERN.search(chunk_text)),\n",
        "                doc_metadata\n",
        "            )\n",
        "\n",
        "    def chunk_document(self, content: str, source_file: str) -> Generator[Chunk, None, None]:\n",
        "        # 1. Extract Metadata first\n",
        "        doc_metadata, body_content = self._parse_frontmatter(content)\n",
        "\n",
        "        # 2. Parse sections from the body only\n",
        "        sections = self.parse_sections(body_content)\n",
        "        chunk_index = 0\n",
        "\n",
        "        for i, section in enumerate(sections):\n",
        "            section_path = self.build_section_path(sections, i)\n",
        "            header = ('#' * section['level'] + ' ' + section['title']) if section['title'] else ''\n",
        "\n",
        "            # Combine header and content\n",
        "            full_text = (header + '\\n\\n' + section['content']).strip() if header else section['content'].strip()\n",
        "            if not full_text: continue\n",
        "\n",
        "            token_count = self.count_tokens(full_text)\n",
        "\n",
        "            # 3. Create Chunks with Metadata\n",
        "            if token_count <= self.max_tokens:\n",
        "                yield Chunk(\n",
        "                    self.generate_chunk_id(source_file, full_text, chunk_index),\n",
        "                    full_text, source_file, section_path, token_count, section['has_table'],\n",
        "                    doc_metadata # Injected here\n",
        "                )\n",
        "                chunk_index += 1\n",
        "            else:\n",
        "                # Pass metadata into the splitter\n",
        "                for chunk in self.split_large_section(section['content'], section_path, source_file, chunk_index, doc_metadata):\n",
        "                    yield chunk\n",
        "                    chunk_index += 1"
      ],
      "metadata": {
        "id": "xsIUVTxhfHcp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb sentence-transformers rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-LWkw9R5lV6q",
        "outputId": "24a18fb4-21df-4b22-ac29-1a730f1543ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading pypika-0.51.0-py2.py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypika-0.51.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypika, durationpy, rank_bm25, pyproject_hooks, pybase64, opentelemetry-proto, humanfriendly, bcrypt, backoff, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.4.1 coloredlogs-15.0.1 durationpy-0.10 humanfriendly-10.0 kubernetes-35.0.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.51.0 pyproject_hooks-1.2.0 rank_bm25-0.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              },
              "id": "0d49443a442f41e1b0b86788fd942c0b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "\n",
        "class HybridIndex:\n",
        "    def __init__(self, collection_name=\"rag_conflict_resolution\"):\n",
        "        # 1. Initialize Dense Model (BGE-M3)\n",
        "        print(\"Loading BGE-M3 model... (this may take a moment)\")\n",
        "        self.dense_model = SentenceTransformer('BAAI/bge-m3')\n",
        "\n",
        "        # 2. Initialize ChromaDB (Vector Store)\n",
        "        # We explicitly reset the client to ensure clean state if re-running\n",
        "        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "\n",
        "        # Delete if exists to avoid \"doubling up\" data during debugging\n",
        "        try:\n",
        "            self.chroma_client.delete_collection(collection_name)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        self.collection = self.chroma_client.create_collection(\n",
        "            name=collection_name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        # 3. Initialize Sparse Storage (In-memory for BM25)\n",
        "        self.bm25 = None\n",
        "        self.chunk_map = {}\n",
        "        self.tokenized_corpus = []\n",
        "\n",
        "    def _tokenize(self, text: str):\n",
        "        \"\"\"Simple regex tokenizer for BM25.\"\"\"\n",
        "        return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def _sanitize_metadata(self, metadata: dict) -> dict:\n",
        "        \"\"\"Converts complex types (dates) in metadata to strings for ChromaDB.\"\"\"\n",
        "        clean_meta = {}\n",
        "        for k, v in metadata.items():\n",
        "            # Convert date/datetime objects to ISO format strings\n",
        "            if isinstance(v, (datetime.date, datetime.datetime)):\n",
        "                clean_meta[k] = v.isoformat()\n",
        "            # Pass through valid types\n",
        "            elif isinstance(v, (str, int, float, bool, type(None))):\n",
        "                clean_meta[k] = v\n",
        "            # Fallback: Stringify anything else (like nested lists)\n",
        "            else:\n",
        "                clean_meta[k] = str(v)\n",
        "        return clean_meta\n",
        "\n",
        "    def add_documents(self, chunks: list):\n",
        "        ids = [chunk.id for chunk in chunks]\n",
        "        texts = [chunk.content for chunk in chunks]\n",
        "\n",
        "        # --- FIX APPLIED HERE: SANITIZE METADATA ---\n",
        "        metadatas = [self._sanitize_metadata(chunk.metadata) for chunk in chunks]\n",
        "\n",
        "        # --- DENSE PIPELINE (BGE-M3) ---\n",
        "        print(f\"Generating dense embeddings for {len(chunks)} chunks...\")\n",
        "        embeddings = self.dense_model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        self.collection.add(\n",
        "            documents=texts,\n",
        "            embeddings=embeddings.tolist(),\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        # --- SPARSE PIPELINE (BM25) ---\n",
        "        print(\"Building BM25 index...\")\n",
        "        new_tokens = [self._tokenize(text) for text in texts]\n",
        "        self.tokenized_corpus.extend(new_tokens)\n",
        "\n",
        "        start_idx = len(self.chunk_map)\n",
        "        for i, chunk_id in enumerate(ids):\n",
        "            self.chunk_map[start_idx + i] = chunk_id\n",
        "\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "        print(f\"Indexing complete. {len(ids)} documents added.\")\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 5):\n",
        "        # 1. Dense Search (Chroma)\n",
        "        query_embedding = self.dense_model.encode([query]).tolist()\n",
        "        dense_results = self.collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=top_k\n",
        "        )\n",
        "\n",
        "        # 2. Sparse Search (BM25)\n",
        "        tokenized_query = self._tokenize(query)\n",
        "        sparse_scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_n_sparse = np.argsort(sparse_scores)[::-1][:top_k]\n",
        "\n",
        "        print(\"--- Dense Results ---\")\n",
        "        for i, doc in enumerate(dense_results['documents'][0]):\n",
        "            print(f\"Doc: {doc[:50]}... | ID: {dense_results['ids'][0][i]}\")\n",
        "\n",
        "        print(\"\\n--- Sparse Results (BM25) ---\")\n",
        "        for idx in top_n_sparse:\n",
        "            chunk_id = self.chunk_map.get(idx)\n",
        "            print(f\"Score: {sparse_scores[idx]:.4f} | ID: {chunk_id}\")\n",
        "\n",
        "        return dense_results, top_n_sparse"
      ],
      "metadata": {
        "id": "RW92D_bsjlwI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def run_indexing_pipeline(input_dir: str = \"dataset\"):\n",
        "    # 1. Initialize Components\n",
        "    # We use a smaller chunk size (500) to create more granular conflict targets\n",
        "    chunker = MarkdownChunker(max_tokens=500)\n",
        "\n",
        "    # Initialize the Hybrid Index (BGE-M3 + BM25)\n",
        "    # Note: If you run this cell twice, it will add duplicate data unless you restart runtime\n",
        "    # or add a line to delete the collection first.\n",
        "    indexer = HybridIndex(collection_name=\"rag_conflict_resolution\")\n",
        "\n",
        "    input_path = Path(input_dir)\n",
        "    all_chunks = []\n",
        "\n",
        "    print(f\"🚀 Starting Indexing Pipeline reading from: {input_path}\")\n",
        "\n",
        "    # 2. Process Each File\n",
        "    for md_file in input_path.glob(\"*.md\"):\n",
        "        print(f\"Processing {md_file.name}...\")\n",
        "\n",
        "        try:\n",
        "            with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Generate chunks for this document\n",
        "            # The chunker handles frontmatter extraction automatically now\n",
        "            file_chunks = list(chunker.chunk_document(content, md_file.name))\n",
        "\n",
        "            # Verify metadata extraction (sanity check)\n",
        "            if file_chunks and not file_chunks[0].metadata:\n",
        "                 print(f\"   ⚠️ Warning: No metadata found for {md_file.name}\")\n",
        "            elif file_chunks:\n",
        "                 print(f\"   ✅ Extracted {len(file_chunks)} chunks. Metadata: {file_chunks[0].metadata.get('doc_id', 'Unknown')}\")\n",
        "\n",
        "            all_chunks.extend(file_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error processing {md_file.name}: {e}\")\n",
        "\n",
        "    # 3. Batch Indexing\n",
        "    if all_chunks:\n",
        "        print(f\"\\n📦 Indexing {len(all_chunks)} total chunks into Hybrid Store...\")\n",
        "        indexer.add_documents(all_chunks)\n",
        "        print(\"🎉 Indexing Complete!\")\n",
        "    else:\n",
        "        print(\"❌ No chunks were generated. Check your input directory.\")\n",
        "\n",
        "    return indexer\n",
        "\n",
        "# --- EXECUTE PIPELINE ---\n",
        "# This runs the whole show and returns the active indexer object\n",
        "rag_index = run_indexing_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGx_oagxlmFo",
        "outputId": "5c0e45d8-66a4-404e-f99d-3ecd1df77def"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BGE-M3 model... (this may take a moment)\n",
            "🚀 Starting Indexing Pipeline reading from: dataset\n",
            "Processing Bihar_Budget_Analysis_2025-26.md...\n",
            "   ✅ Extracted 26 chunks. Metadata: budgetReport25-26\n",
            "Processing State Budget Analysis - Bihar 2019-20 English.md...\n",
            "   ✅ Extracted 26 chunks. Metadata: budgetReport19-20\n",
            "Processing Bihar_Budget_Analysis_2024-25.md...\n",
            "   ✅ Extracted 26 chunks. Metadata: budgetReport24-25\n",
            "Processing Bihar_Budget_Analysis_2023-24.md...\n",
            "   ✅ Extracted 26 chunks. Metadata: budgetReport23-24\n",
            "Processing Bihar Budget Analysis 2022-23.md...\n",
            "   ✅ Extracted 25 chunks. Metadata: budgetReport22-23\n",
            "Processing State Budget Analysis - Bihar 2020-21.md...\n",
            "   ✅ Extracted 30 chunks. Metadata: budgetReport20-21\n",
            "Processing Bihar Budget Analysis 2016-17.md...\n",
            "   ✅ Extracted 12 chunks. Metadata: budgetReport16-17\n",
            "Processing Bihar Budget Analysis - 2018-19.md...\n",
            "   ✅ Extracted 20 chunks. Metadata: budgetReport18-19\n",
            "Processing Bihar Budget Analysis 2017-18.md...\n",
            "   ✅ Extracted 20 chunks. Metadata: budgetReport17-18\n",
            "Processing State_Budget_Analysis_2021-22-Bihar.md...\n",
            "   ✅ Extracted 32 chunks. Metadata: budgetReport21-22\n",
            "\n",
            "📦 Indexing 243 total chunks into Hybrid Store...\n",
            "Generating dense embeddings for 243 chunks...\n",
            "Building BM25 index...\n",
            "Indexing complete. 243 documents added.\n",
            "🎉 Indexing Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Query\n",
        "test_query = \"What was the finance minister in 2020?\"\n",
        "\n",
        "print(f\"🔎 Testing Query: '{test_query}'\\n\")\n",
        "dense_results, sparse_indices = rag_index.hybrid_search(test_query, top_k=3)\n",
        "\n",
        "# The output above will show you:\n",
        "# 1. The retrieved text\n",
        "# 2. The IDs (which you can trace back to specific chunks)\n",
        "# 3. Dense vs Sparse scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ozX9xfmFu0",
        "outputId": "6481788a-8686-4b8d-fbd1-efa326cc48e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Testing Query: 'What was the finance minister in 2020?'\n",
            "\n",
            "--- Dense Results ---\n",
            "Doc: ## Bihar Budget Analysis 2020-21\n",
            "\n",
            "The Finance Mini... | ID: 89ce7923510f0204\n",
            "Doc: ## Bihar Budget Analysis 2019-20\n",
            "\n",
            "The Finance Mini... | ID: 518cf4af85581b05\n",
            "Doc: ## Bihar Budget Analysis 2021-22\n",
            "\n",
            "The Finance Mini... | ID: ea5f578cd96bf4ef\n",
            "\n",
            "--- Sparse Results (BM25) ---\n",
            "Score: 13.3674 | ID: ea5f578cd96bf4ef\n",
            "Score: 11.2771 | ID: 89ce7923510f0204\n",
            "Score: 10.3157 | ID: d56b1b5e232a76f3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "class ConflictEntryGate:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        # Pairs of keywords that suggest conflict\n",
        "        self.polarity_pairs = [\n",
        "            ({\"allowed\", \"may\", \"can\"}, {\"prohibited\", \"must not\", \"cannot\", \"banned\"}),\n",
        "            ({\"required\", \"must\", \"shall\"}, {\"optional\", \"suggested\", \"recommended\"}),\n",
        "            ({\"increase\", \"rise\", \"growth\"}, {\"decrease\", \"fall\", \"decline\", \"reduction\"}),\n",
        "            ({\"surplus\", \"profit\"}, {\"deficit\", \"loss\"})\n",
        "        ]\n",
        "\n",
        "    def _check_polarity(self, texts: list[str]) -> bool:\n",
        "        \"\"\"Checks if opposing keywords exist across the retrieved set.\"\"\"\n",
        "        combined_text = \" \".join(texts).lower()\n",
        "\n",
        "        for set_a, set_b in self.polarity_pairs:\n",
        "            has_a = any(w in combined_text for w in set_a)\n",
        "            has_b = any(w in combined_text for w in set_b)\n",
        "            if has_a and has_b:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _check_semantic_overlap(self, chunks: list, threshold: float = 0.75) -> bool:\n",
        "        \"\"\"\n",
        "        Calculates pairwise similarity. Returns True if two chunks from\n",
        "        DIFFERENT documents have high semantic similarity.\n",
        "        \"\"\"\n",
        "        if len(chunks) < 2:\n",
        "            return False\n",
        "\n",
        "        # Re-embed top-k chunks to calculate pairwise similarity matrix\n",
        "        # (Fast for small k)\n",
        "        texts = [c['content'] for c in chunks]\n",
        "        embeddings = self.embedding_model.encode(texts)\n",
        "        sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "        # Check upper triangle of matrix\n",
        "        for i in range(len(chunks)):\n",
        "            for j in range(i + 1, len(chunks)):\n",
        "                sim_score = sim_matrix[i][j]\n",
        "\n",
        "                # If highly similar...\n",
        "                if sim_score > threshold:\n",
        "                    doc_a = chunks[i]['metadata'].get('doc_id')\n",
        "                    doc_b = chunks[j]['metadata'].get('doc_id')\n",
        "\n",
        "                    # ...and from different documents\n",
        "                    if doc_a != doc_b:\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    def evaluate(self, retrieved_chunks: list) -> dict:\n",
        "        \"\"\"\n",
        "        Main Gate Logic.\n",
        "        Input: list of dicts {'content': str, 'metadata': dict}\n",
        "        \"\"\"\n",
        "        result = {\n",
        "            \"trigger_conflict_pipeline\": False,\n",
        "            \"reasons\": []\n",
        "        }\n",
        "\n",
        "        # Guardrail 1: Empty results\n",
        "        if not retrieved_chunks:\n",
        "            return result\n",
        "\n",
        "        # Guardrail 2: Document Diversity (Must have >1 source to conflict)\n",
        "        doc_ids = {c['metadata'].get('doc_id') for c in retrieved_chunks}\n",
        "        if len(doc_ids) < 2:\n",
        "            return result # Normal RAG path\n",
        "\n",
        "        result['reasons'].append(f\"Multiple sources detected: {doc_ids}\")\n",
        "\n",
        "        # Signal 1: Polarity Keywords\n",
        "        texts = [c['content'] for c in retrieved_chunks]\n",
        "        if self._check_polarity(texts):\n",
        "            result['trigger_conflict_pipeline'] = True\n",
        "            result['reasons'].append(\"Opposing polarity keywords detected\")\n",
        "\n",
        "        # Signal 2: High Semantic Overlap between documents\n",
        "        # (Only run if not already triggered, or run anyway to add evidence)\n",
        "        if self._check_semantic_overlap(retrieved_chunks):\n",
        "            result['trigger_conflict_pipeline'] = True\n",
        "            result['reasons'].append(\"High semantic overlap between different documents\")\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "KghozUvumqEM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instantiate the Gate\n",
        "gate = ConflictEntryGate(rag_index.dense_model)\n",
        "\n",
        "# 2. Helper function to format search results for the Gate\n",
        "def get_chunks_from_search(dense_res, sparse_indices):\n",
        "    \"\"\"Merges and formats results into a clean list of dicts.\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Process Dense Results\n",
        "    # Note: Chroma returns lists of lists\n",
        "    if dense_res['documents']:\n",
        "        for i, text in enumerate(dense_res['documents'][0]):\n",
        "            chunks.append({\n",
        "                'content': text,\n",
        "                'metadata': dense_res['metadatas'][0][i],\n",
        "                'source_type': 'dense'\n",
        "            })\n",
        "\n",
        "    # Process Sparse Results (Retrieve from Indexer memory)\n",
        "    # We limit to top 3 sparse to avoid noise\n",
        "    for idx in sparse_indices[:3]:\n",
        "        chunk_id = rag_index.chunk_map.get(idx)\n",
        "        # Find the chunk object (This is a bit inefficient, in prod we'd use a doc store)\n",
        "        # For this prototype, we'll skip adding sparse TEXT if not easily available,\n",
        "        # but usually you'd query the DB by ID.\n",
        "        # Let's rely on Dense results for the Gate test for now.\n",
        "        pass\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# 3. Run a Test Case\n",
        "print(\"--- TEST: Conflict Gate ---\")\n",
        "query = \"What is the fiscal deficit limit?\"\n",
        "\n",
        "# Perform Search\n",
        "dense, sparse = rag_index.hybrid_search(query, top_k=5)\n",
        "formatted_chunks = get_chunks_from_search(dense, sparse)\n",
        "\n",
        "# Check Gate\n",
        "decision = gate.evaluate(formatted_chunks)\n",
        "\n",
        "print(\"\\n--- GATE DECISION ---\")\n",
        "print(f\"Trigger Conflict Pipeline: {decision['trigger_conflict_pipeline']}\")\n",
        "print(f\"Reasons: {decision['reasons']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AiV4L5Hnr31",
        "outputId": "b13d7a76-3fb7-4071-9f04-9fa575e41c2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: Conflict Gate ---\n",
            "--- Dense Results ---\n",
            "Doc: # Deficits, Debts and FRBM Targets for 2017-18\n",
            "\n",
            "Th... | ID: aedf89e5d3586f1a\n",
            "Doc: # Deficits, Debts and FRBM Targets for 2018-19\n",
            "\n",
            "Th... | ID: 9df11ed5a4c388a8\n",
            "Doc: # Deficits and Debt Targets for 2024-25\n",
            "\n",
            "The Bihar... | ID: 57cec7748e55d98d\n",
            "Doc: # Deficits, Debts and FRBM Targets for 20 19-20\n",
            "\n",
            "T... | ID: ac029d0e2a90267c\n",
            "Doc: ## Deficits, Debts and FRBM Targets for 2020-21\n",
            "\n",
            "T... | ID: f5db1ab57b111635\n",
            "\n",
            "--- Sparse Results (BM25) ---\n",
            "Score: 11.2776 | ID: 43db0bda5149eff1\n",
            "Score: 11.1785 | ID: bf339162c892f15c\n",
            "Score: 11.1718 | ID: bb202ea0741a6e3c\n",
            "Score: 11.0643 | ID: aedf89e5d3586f1a\n",
            "Score: 11.0431 | ID: 57cec7748e55d98d\n",
            "\n",
            "--- GATE DECISION ---\n",
            "Trigger Conflict Pipeline: True\n",
            "Reasons: [\"Multiple sources detected: {'budgetReport17-18', 'budgetReport18-19', 'budgetReport20-21', 'budgetReport19-20', 'budgetReport24-25'}\", 'Opposing polarity keywords detected', 'High semantic overlap between different documents']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Updated for newer NLTK versions"
      ],
      "metadata": {
        "id": "hts4uh81oLhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ParsedClaim:\n",
        "    text: str\n",
        "    source: str\n",
        "    date: str\n",
        "    doc_id: str\n",
        "    vector: np.array\n",
        "    original_chunk_id: str\n",
        "\n",
        "class ConflictDetector:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        # Keywords that suggest a sentence is a \"Claim\" (Spec: Section 2)\n",
        "        self.claim_markers = [\n",
        "            \"must\", \"shall\", \"may\", \"cannot\", \"must not\", \"allowed\", \"prohibited\",\n",
        "            \"requires\", \"applies\", \"target\", \"estimated\", \"forecast\", \"goal\", \"limit\"\n",
        "        ]\n",
        "\n",
        "        # Opposing polarity map\n",
        "        self.polarity_map = {\n",
        "            \"allowed\": \"prohibited\", \"prohibited\": \"allowed\",\n",
        "            \"must\": \"must not\", \"must not\": \"must\",\n",
        "            \"increase\": \"decrease\", \"decrease\": \"increase\",\n",
        "            \"surplus\": \"deficit\", \"deficit\": \"surplus\"\n",
        "        }\n",
        "\n",
        "    def _extract_numeric(self, text: str):\n",
        "        \"\"\"\n",
        "        Parses numbers and units.\n",
        "        Matches: \"5.32%\", \"Rs 14,649 crore\", \"3%\"\n",
        "        \"\"\"\n",
        "        # Regex to find number + optional unit\n",
        "        # Pattern: (digits with commas/decimals) + whitespace + (optional word/symbol)\n",
        "        pattern = r'(\\d+(?:,\\d+)*(?:\\.\\d+)?)\\s*([%a-zA-Z]+)?'\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        results = []\n",
        "        for num_str, unit in matches:\n",
        "            # Clean number (remove commas)\n",
        "            try:\n",
        "                val = float(num_str.replace(',', ''))\n",
        "                results.append({'val': val, 'unit': unit.strip().lower() if unit else None})\n",
        "            except:\n",
        "                continue\n",
        "        return results\n",
        "\n",
        "    def _detect_numeric_conflict(self, claims: list[ParsedClaim]) -> list:\n",
        "        \"\"\"\n",
        "        Spec Section 4c: Checks for value mismatches in same-unit claims.\n",
        "        \"\"\"\n",
        "        conflicts = []\n",
        "        # Simple pairwise comparison within the cluster\n",
        "        for i in range(len(claims)):\n",
        "            for j in range(i + 1, len(claims)):\n",
        "                c1, c2 = claims[i], claims[j]\n",
        "\n",
        "                nums1 = self._extract_numeric(c1.text)\n",
        "                nums2 = self._extract_numeric(c2.text)\n",
        "\n",
        "                # If both have numbers, compare them\n",
        "                for n1 in nums1:\n",
        "                    for n2 in nums2:\n",
        "                        # Check unit compatibility (must be same unit or both empty)\n",
        "                        if n1['unit'] == n2['unit']:\n",
        "                            # Check value difference (> 5% diff is conflict)\n",
        "                            # Avoid div by zero\n",
        "                            if n1['val'] == 0 and n2['val'] == 0: continue\n",
        "\n",
        "                            diff = abs(n1['val'] - n2['val'])\n",
        "                            base = max(abs(n1['val']), abs(n2['val']))\n",
        "\n",
        "                            if diff / base > 0.05: # 5% tolerance\n",
        "                                conflicts.append({\n",
        "                                    \"type\": \"numeric_conflict\",\n",
        "                                    \"description\": f\"Value mismatch: {n1['val']} vs {n2['val']} ({n1['unit']})\",\n",
        "                                    \"claim_a\": c1,\n",
        "                                    \"claim_b\": c2\n",
        "                                })\n",
        "        return conflicts\n",
        "\n",
        "    def _detect_temporal_conflict(self, claims: list[ParsedClaim]) -> list:\n",
        "        \"\"\"\n",
        "        Spec Section 4b: Same Doc ID, Different Dates/Versions -> Contradiction\n",
        "        \"\"\"\n",
        "        conflicts = []\n",
        "        for i in range(len(claims)):\n",
        "            for j in range(i + 1, len(claims)):\n",
        "                c1, c2 = claims[i], claims[j]\n",
        "\n",
        "                # Must be same Document ID (e.g., \"policy_access\")\n",
        "                if c1.doc_id == c2.doc_id:\n",
        "                    # But different versions/dates\n",
        "                    if c1.date != c2.date:\n",
        "                        conflicts.append({\n",
        "                            \"type\": \"temporal_conflict\",\n",
        "                            \"description\": f\"Version mismatch within {c1.doc_id}: {c1.date} vs {c2.date}\",\n",
        "                            \"claim_a\": c1,\n",
        "                            \"claim_b\": c2\n",
        "                        })\n",
        "        return conflicts\n",
        "\n",
        "    def _cluster_claims(self, claims: list[ParsedClaim], threshold=0.65):\n",
        "        \"\"\"\n",
        "        Groups claims by semantic similarity so we only compare relevant things.\n",
        "        \"\"\"\n",
        "        if len(claims) < 2:\n",
        "            return [[c] for c in claims]\n",
        "\n",
        "        vectors = np.array([c.vector for c in claims])\n",
        "\n",
        "        # Agglomerative Clustering based on Cosine Distance\n",
        "        # 1 - threshold because distance = 1 - similarity\n",
        "        clustering = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            metric='cosine',\n",
        "            linkage='average',\n",
        "            distance_threshold=1 - threshold\n",
        "        )\n",
        "        labels = clustering.fit_predict(vectors)\n",
        "\n",
        "        clusters = {}\n",
        "        for idx, label in enumerate(labels):\n",
        "            if label not in clusters: clusters[label] = []\n",
        "            clusters[label].append(claims[idx])\n",
        "\n",
        "        return list(clusters.values())\n",
        "\n",
        "    def detect_conflicts(self, retrieved_chunks: list) -> dict:\n",
        "        \"\"\"\n",
        "        Main Pipeline:\n",
        "        1. Split Chunks -> Sentences\n",
        "        2. Filter for Claims\n",
        "        3. Cluster Claims\n",
        "        4. Run Auditors (Numeric, Temporal)\n",
        "        \"\"\"\n",
        "        all_claims = []\n",
        "\n",
        "        # --- Stage 1 & 2: Evidence Grouping & Claim Extraction ---\n",
        "        for chunk in retrieved_chunks:\n",
        "            sentences = sent_tokenize(chunk['content'])\n",
        "\n",
        "            # Embed all sentences at once for speed\n",
        "            if not sentences: continue\n",
        "            vectors = self.embedding_model.encode(sentences)\n",
        "\n",
        "            for text, vector in zip(sentences, vectors):\n",
        "                # Filter: Is this a claim?\n",
        "                if any(marker in text.lower() for marker in self.claim_markers):\n",
        "                    all_claims.append(ParsedClaim(\n",
        "                        text=text,\n",
        "                        source=chunk['metadata'].get('source', 'unknown'),\n",
        "                        date=chunk['metadata'].get('date', 'unknown'),\n",
        "                        doc_id=chunk['metadata'].get('doc_id', 'unknown'),\n",
        "                        vector=vector,\n",
        "                        original_chunk_id=chunk.get('id', 'unknown')\n",
        "                    ))\n",
        "\n",
        "        if not all_claims:\n",
        "            return {\"conflict_detected\": False, \"reason\": \"No claims extracted\"}\n",
        "\n",
        "        # --- Stage 3: Claim Clustering ---\n",
        "        # We group claims so we don't compare \"Admin Policy\" numbers with \"HR Policy\" numbers\n",
        "        claim_clusters = self._cluster_claims(all_claims)\n",
        "\n",
        "        detected_conflicts = []\n",
        "\n",
        "        # --- Stage 4: Conflict Typing ---\n",
        "        for cluster_id, cluster in enumerate(claim_clusters):\n",
        "            # Check Numeric\n",
        "            nums = self._detect_numeric_conflict(cluster)\n",
        "            if nums:\n",
        "                for c in nums: c['cluster_id'] = cluster_id\n",
        "                detected_conflicts.extend(nums)\n",
        "\n",
        "            # Check Temporal\n",
        "            temps = self._detect_temporal_conflict(cluster)\n",
        "            if temps:\n",
        "                for c in temps: c['cluster_id'] = cluster_id\n",
        "                detected_conflicts.extend(temps)\n",
        "\n",
        "            # (Polarity check could be added here similar to logic above)\n",
        "\n",
        "        # Structure Output\n",
        "        if not detected_conflicts:\n",
        "            return {\"conflict_detected\": False}\n",
        "\n",
        "        return {\n",
        "            \"conflict_detected\": True,\n",
        "            \"count\": len(detected_conflicts),\n",
        "            \"conflicts\": [\n",
        "                {\n",
        "                    \"type\": c['type'],\n",
        "                    \"cluster_id\": c['cluster_id'],\n",
        "                    \"description\": c['description'],\n",
        "                    \"claim_1\": {\n",
        "                        \"text\": c['claim_a'].text,\n",
        "                        \"source\": c['claim_a'].source,\n",
        "                        \"date\": c['claim_a'].date\n",
        "                    },\n",
        "                    \"claim_2\": {\n",
        "                        \"text\": c['claim_b'].text,\n",
        "                        \"source\": c['claim_b'].source,\n",
        "                        \"date\": c['claim_b'].date\n",
        "                    }\n",
        "                }\n",
        "                for c in detected_conflicts\n",
        "            ]\n",
        "        }"
      ],
      "metadata": {
        "id": "EfTyudytnuO_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "6-9-IvLSoV1V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "class LLMNarrator:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-3-flash-preview\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # We define the strict persona here.\n",
        "        # This is the \"North Star\" invariant from your spec.\n",
        "        self.system_instruction = \"\"\"\n",
        "        You are a generic Conflict Narrator for a corporate RAG system.\n",
        "        Your ONLY job is to explain disagreements found in the provided JSON data.\n",
        "\n",
        "        CRITICAL RULES:\n",
        "        1. NEVER resolve the conflict. Do not try to guess which document is correct.\n",
        "        2. If Source A says \"3%\" and Source B says \"5%\", you must explicitly state:\n",
        "           \"Source A (dated X) claims 3%, while Source B (dated Y) claims 5%.\"\n",
        "        3. Do not synthesize or average numbers.\n",
        "        4. If the conflict is temporal (old vs new), explicitly point out the dates.\n",
        "        5. Use bullet points for clarity.\n",
        "        \"\"\"\n",
        "\n",
        "    def generate(self, context_prompt: str, user_content: str) -> str:\n",
        "        \"\"\"\n",
        "        Generates the response using Gemini 1.5 Flash.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Initialize model with the specific system instruction\n",
        "            model = genai.GenerativeModel(\n",
        "                model_name=self.model_name,\n",
        "                system_instruction=self.system_instruction\n",
        "            )\n",
        "\n",
        "            # Combine the specific context (conflict JSON or chunks) with the user request\n",
        "            full_prompt = f\"{context_prompt}\\n\\n{user_content}\"\n",
        "\n",
        "            response = model.generate_content(full_prompt)\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Gemini Error: {str(e)}\"\n",
        "\n",
        "    def construct_conflict_prompt(self, conflict_data: dict) -> tuple[str, str]:\n",
        "        \"\"\"Formats the JSON conflict report for the LLM.\"\"\"\n",
        "        context_prompt = \"CONFLICT DETECTED. The following JSON object describes a contradiction in the retrieved documents.\"\n",
        "\n",
        "        user_content = f\"\"\"\n",
        "        Structured Conflict Data:\n",
        "        {json.dumps(conflict_data, indent=2)}\n",
        "\n",
        "        Task:\n",
        "        Explain this conflict to the user. Cite the 'source' and 'date' for every opposing claim.\n",
        "        \"\"\"\n",
        "        return context_prompt, user_content\n",
        "\n",
        "    def construct_normal_prompt(self, chunks: list, query: str) -> tuple[str, str]:\n",
        "        \"\"\"Standard RAG prompt.\"\"\"\n",
        "        context_prompt = \"Answer the user's question based ONLY on the context provided below.\"\n",
        "\n",
        "        context_text = \"\\n\\n\".join([\n",
        "            f\"--- Source: {c['metadata'].get('doc_id')} (Date: {c['metadata'].get('date')}) ---\\n{c['content']}\"\n",
        "            for c in chunks\n",
        "        ])\n",
        "\n",
        "        user_content = f\"\"\"\n",
        "        Context:\n",
        "        {context_text}\n",
        "\n",
        "        Question: {query}\n",
        "        \"\"\"\n",
        "        return context_prompt, user_content"
      ],
      "metadata": {
        "id": "bzpGdbpMpGhP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConflictAwareRAG:\n",
        "    def __init__(self, indexer, gate, detector, llm_narrator):\n",
        "        self.indexer = indexer\n",
        "        self.gate = gate\n",
        "        self.detector = detector\n",
        "        self.llm = llm_narrator\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        print(f\"🔹 Processing Query: '{query}'\")\n",
        "\n",
        "        # 1. RETRIEVAL (Hybrid)\n",
        "        dense_res, sparse_indices = self.indexer.hybrid_search(query, top_k=5)\n",
        "\n",
        "        # Helper to standardize chunks\n",
        "        retrieved_chunks = []\n",
        "        if dense_res['documents']:\n",
        "            for i, text in enumerate(dense_res['documents'][0]):\n",
        "                retrieved_chunks.append({\n",
        "                    'id': dense_res['ids'][0][i],\n",
        "                    'content': text,\n",
        "                    'metadata': dense_res['metadatas'][0][i]\n",
        "                })\n",
        "\n",
        "        # 2. CONFLICT ENTRY GATE\n",
        "        gate_decision = self.gate.evaluate(retrieved_chunks)\n",
        "\n",
        "        if not gate_decision['trigger_conflict_pipeline']:\n",
        "            print(\"✅ Gate Passed: No obvious conflict. Proceeding to Standard RAG.\")\n",
        "            sys_prompt, user_msg = self.llm.construct_normal_prompt(retrieved_chunks, query)\n",
        "            answer = self.llm.generate(sys_prompt, user_msg)\n",
        "            return {\"type\": \"standard_answer\", \"content\": answer}\n",
        "\n",
        "        # 3. CONFLICT DETECTION (If Gate Triggered)\n",
        "        print(f\"⚠️ Gate Triggered! Reasons: {gate_decision['reasons']}\")\n",
        "        print(\"   -> Running Conflict Detection Engine...\")\n",
        "\n",
        "        conflict_report = self.detector.detect_conflicts(retrieved_chunks)\n",
        "\n",
        "        # 4. FINAL OUTPUT GENERATION\n",
        "        if conflict_report['conflict_detected']:\n",
        "            print(f\"   🚨 CONFLICT CONFIRMED. Found {conflict_report['count']} disagreements.\")\n",
        "\n",
        "            # Construct the \"North Star\" Structured Output\n",
        "            # (The detector already outputs the dict, we just pass it to the LLM)\n",
        "            sys_prompt, user_msg = self.llm.construct_conflict_prompt(conflict_report)\n",
        "            explanation = self.llm.generate(sys_prompt, user_msg)\n",
        "\n",
        "            return {\n",
        "                \"type\": \"conflict_exposed\",\n",
        "                \"structured_data\": conflict_report, # The raw JSON for the UI/Debug\n",
        "                \"narrative_explanation\": explanation # The LLM text\n",
        "            }\n",
        "        else:\n",
        "            print(\"   ℹ️ False Alarm: Gate triggered, but deeper analysis found no specific claim conflicts.\")\n",
        "            # Fallback to standard RAG\n",
        "            sys_prompt, user_msg = self.llm.construct_normal_prompt(retrieved_chunks, query)\n",
        "            answer = self.llm.generate(sys_prompt, user_msg)\n",
        "            return {\"type\": \"standard_answer\", \"content\": answer}"
      ],
      "metadata": {
        "id": "JgIb_sU7pkyx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup (Assuming 'rag_index' from previous steps is ready)\n",
        "# Re-instantiate components to ensure they are linked correctly\n",
        "gate = ConflictEntryGate(rag_index.dense_model)\n",
        "detector = ConflictDetector(rag_index.dense_model)\n",
        "llm = LLMNarrator(api_key=\"AIzaSyBCfrxnkofU6yMoI6_VAvu_nzWmQqkx6nU\") # Pass your key here: api_key=\"sk-...\"\n",
        "\n",
        "# 2. Build the Engine\n",
        "rag_engine = ConflictAwareRAG(rag_index, gate, detector, llm)\n",
        "\n",
        "# 3. Run Test 1: Expect Conflict (if your data has diverging numbers)\n",
        "print(\"\\n--- TEST CASE 1: Numeric/Temporal Conflict ---\")\n",
        "result_1 = rag_engine.process_query(\"What was GSDP in the year 2022-23?\")\n",
        "\n",
        "print(\"\\n--- FINAL OUTPUT ---\")\n",
        "if result_1['type'] == 'conflict_exposed':\n",
        "    print(\"JSON OUTPUT (Machine Readable):\")\n",
        "    print(json.dumps(result_1['structured_data'], indent=2))\n",
        "    print(\"\\nNARRATIVE OUTPUT (Human Readable):\")\n",
        "    print(result_1['narrative_explanation'])\n",
        "else:\n",
        "    print(result_1['content'])"
      ],
      "metadata": {
        "id": "3KHKZEsZpqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyvVtSmgp_lR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}