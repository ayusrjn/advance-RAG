{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIDan8zd5f16"
      },
      "outputs": [],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# 1. Setup paths\n",
        "input_dir = Path(\"/content/pdfs\")\n",
        "output_dir = Path(\"extracted_md\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# 2. Initialize Docling Converter\n",
        "converter = DocumentConverter()\n",
        "\n",
        "def extract_pdfs():\n",
        "    pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {input_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} files. Starting extraction...\")\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "        try:\n",
        "            print(f\"Processing: {pdf_path.name}...\")\n",
        "\n",
        "            # Convert PDF to Docling's internal format\n",
        "            result = converter.convert(pdf_path)\n",
        "\n",
        "            # Export to Markdown (best for your chunking strategy)\n",
        "            md_output = result.document.export_to_markdown()\n",
        "\n",
        "            # Save the file\n",
        "            output_filename = pdf_path.stem + \".md\"\n",
        "            with open(output_dir / output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(md_output)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "    print(f\"\\nSuccess! Extracted files are in: {output_dir}\")\n",
        "\n",
        "extract_pdfs()\n",
        ""
      ],
      "metadata": {
        "id": "p5enD4MQ5zA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers chromadb tiktoken"
      ],
      "metadata": {
        "id": "zWkesc9YDDE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rank_bm25"
      ],
      "metadata": {
        "id": "G4Pjz4LtENt9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import hashlib\n",
        "import chromadb\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass, asdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. Chunking & Data Structures ---\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    id: str\n",
        "    content: str\n",
        "    source_file: str\n",
        "    section_path: list[str]\n",
        "    token_count: int\n",
        "    has_table: bool\n",
        "\n",
        "class MarkdownChunker:\n",
        "    \"\"\"(Same robust chunker as before)\"\"\"\n",
        "    HEADER_PATTERN = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n",
        "    TABLE_PATTERN = re.compile(r'^\\|.+\\|(?:\\n^\\|.+\\|$)+', re.MULTILINE)\n",
        "\n",
        "    def __init__(self, max_tokens: int = 512, model: str = \"cl100k_base\"):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.tokenizer = tiktoken.get_encoding(model)\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(self.tokenizer.encode(text))\n",
        "\n",
        "    def generate_chunk_id(self, source: str, content: str, index: int) -> str:\n",
        "        hash_input = f\"{source}:{index}:{content[:100]}\"\n",
        "        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]\n",
        "\n",
        "    def parse_sections(self, content: str) -> list[dict]:\n",
        "        lines = content.split('\\n')\n",
        "        sections = []\n",
        "        current_section = {'level': 0, 'title': '', 'content_lines': [], 'has_table': False}\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            line = lines[i]\n",
        "            header_match = self.HEADER_PATTERN.match(line)\n",
        "            if header_match:\n",
        "                if current_section['content_lines'] or current_section['title']:\n",
        "                    section_content = '\\n'.join(current_section['content_lines'])\n",
        "                    current_section['has_table'] = bool(self.TABLE_PATTERN.search(section_content))\n",
        "                    sections.append({\n",
        "                        'level': current_section['level'],\n",
        "                        'title': current_section['title'],\n",
        "                        'content': section_content.strip(),\n",
        "                        'has_table': current_section['has_table']\n",
        "                    })\n",
        "                level = len(header_match.group(1))\n",
        "                title = header_match.group(2).strip()\n",
        "                current_section = {'level': level, 'title': title, 'content_lines': [], 'has_table': False}\n",
        "            else:\n",
        "                current_section['content_lines'].append(line)\n",
        "            i += 1\n",
        "        if current_section['content_lines'] or current_section['title']:\n",
        "            section_content = '\\n'.join(current_section['content_lines'])\n",
        "            sections.append({\n",
        "                'level': current_section['level'],\n",
        "                'title': current_section['title'],\n",
        "                'content': section_content.strip(),\n",
        "                'has_table': bool(self.TABLE_PATTERN.search(section_content))\n",
        "            })\n",
        "        return sections\n",
        "\n",
        "    def build_section_path(self, sections: list[dict], current_idx: int) -> list[str]:\n",
        "        current = sections[current_idx]\n",
        "        path = []\n",
        "        if current['title']: path.append(current['title'])\n",
        "        current_level = current['level']\n",
        "        for i in range(current_idx - 1, -1, -1):\n",
        "            section = sections[i]\n",
        "            if section['level'] < current_level and section['title']:\n",
        "                path.insert(0, section['title'])\n",
        "                current_level = section['level']\n",
        "                if current_level == 1: break\n",
        "        return path\n",
        "\n",
        "    def _build_header_prefix(self, section_path: list[str]) -> str:\n",
        "        if not section_path: return \"\"\n",
        "        lines = []\n",
        "        for i, title in enumerate(section_path):\n",
        "            lines.append('#' * (i + 1) + ' ' + title)\n",
        "        return '\\n'.join(lines) + '\\n\\n'\n",
        "\n",
        "    def split_large_section(self, text: str, section_path: list[str], source_file: str, start_index: int):\n",
        "        header_prefix = self._build_header_prefix(section_path)\n",
        "        header_tokens = self.count_tokens(header_prefix)\n",
        "        available_tokens = self.max_tokens - header_tokens\n",
        "        paragraphs = re.split(r'\\n\\n+', text)\n",
        "        current_chunk_parts = []\n",
        "        current_tokens = 0\n",
        "        chunk_index = start_index\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if not para: continue\n",
        "            para_tokens = self.count_tokens(para)\n",
        "            if current_tokens + para_tokens > available_tokens:\n",
        "                if current_chunk_parts:\n",
        "                    content = header_prefix + '\\n\\n'.join(current_chunk_parts)\n",
        "                    yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), False)\n",
        "                    chunk_index += 1\n",
        "                    current_chunk_parts = []\n",
        "                    current_tokens = 0\n",
        "            current_chunk_parts.append(para)\n",
        "            current_tokens += para_tokens\n",
        "\n",
        "        if current_chunk_parts:\n",
        "            content = header_prefix + '\\n\\n'.join(current_chunk_parts)\n",
        "            yield Chunk(self.generate_chunk_id(source_file, content, chunk_index), content, source_file, section_path, self.count_tokens(content), False)\n",
        "\n",
        "    def chunk_document(self, content: str, source_file: str):\n",
        "        sections = self.parse_sections(content)\n",
        "        chunk_index = 0\n",
        "        for i, section in enumerate(sections):\n",
        "            section_path = self.build_section_path(sections, i)\n",
        "            header_str = self._build_header_prefix(section_path)\n",
        "            full_text = (header_str + section['content']).strip()\n",
        "            token_count = self.count_tokens(full_text)\n",
        "\n",
        "            if token_count <= self.max_tokens:\n",
        "                yield Chunk(self.generate_chunk_id(source_file, full_text, chunk_index), full_text, source_file, section_path, token_count, section['has_table'])\n",
        "                chunk_index += 1\n",
        "            else:\n",
        "                for chunk in self.split_large_section(section['content'], section_path, source_file, chunk_index):\n",
        "                    yield chunk\n",
        "                    chunk_index += 1\n",
        "\n",
        "# --- 2. Hybrid System Class ---\n",
        "\n",
        "class HybridRAG:\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        # Dense Setup\n",
        "        print(f\"üîÑ Loading Dense Model: {model_name}...\")\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self.chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "        self.collection = self.chroma_client.get_or_create_collection(name=\"doc_chunks\")\n",
        "\n",
        "        # Sparse Setup (BM25)\n",
        "        # We must keep an in-memory map of ID -> Tokenized Text for BM25\n",
        "        self.chunk_registry: Dict[str, str] = {}\n",
        "        self.bm25 = None\n",
        "\n",
        "        self.chunker = MarkdownChunker(max_tokens=512)\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple tokenizer for BM25. Can be replaced with spacy or others.\"\"\"\n",
        "        return text.lower().split()\n",
        "\n",
        "    def embed_and_store(self, markdown_text: str, source_name: str):\n",
        "        \"\"\"\n",
        "        1. Chunks document\n",
        "        2. Uploads to Chroma (Dense)\n",
        "        3. Updates local registry for BM25 (Sparse)\n",
        "        \"\"\"\n",
        "        chunks = list(self.chunker.chunk_document(markdown_text, source_name))\n",
        "        if not chunks: return\n",
        "\n",
        "        ids = [c.id for c in chunks]\n",
        "        documents = [c.content for c in chunks]\n",
        "        metadatas = [{\n",
        "            \"source\": c.source_file,\n",
        "            \"token_count\": c.token_count,\n",
        "            \"has_table\": c.has_table,\n",
        "            \"section_path\": \" > \".join(c.section_path)\n",
        "        } for c in chunks]\n",
        "\n",
        "        # 1. Update In-Memory Registry (for BM25 later)\n",
        "        for c in chunks:\n",
        "            self.chunk_registry[c.id] = c.content\n",
        "\n",
        "        # 2. Update Chroma (Dense)\n",
        "        embeddings = self.embedding_model.encode(documents, convert_to_tensor=False).tolist()\n",
        "        self.collection.upsert(ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas)\n",
        "\n",
        "        # Note: We do NOT rebuild BM25 here. It's too slow to do per-file.\n",
        "        # Call build_bm25_index() after all files are loaded.\n",
        "\n",
        "    def build_bm25_index(self):\n",
        "        \"\"\"Must be called after all documents are ingested.\"\"\"\n",
        "        print(f\"üèóÔ∏è Building BM25 Index for {len(self.chunk_registry)} chunks...\")\n",
        "\n",
        "        # Create parallel lists of IDs and Tokenized Docs\n",
        "        self.bm25_ids = list(self.chunk_registry.keys())\n",
        "        corpus = [self._tokenize(self.chunk_registry[doc_id]) for doc_id in self.bm25_ids]\n",
        "\n",
        "        self.bm25 = BM25Okapi(corpus)\n",
        "        print(\"‚úÖ BM25 Index Ready.\")\n",
        "\n",
        "    def hybrid_query(self, query_text: str, top_k: int = 5, alpha: float = 0.5):\n",
        "        \"\"\"\n",
        "        Performs Hybrid Search using Weighted Scoring.\n",
        "        Alpha 1.0 = Pure Dense\n",
        "        Alpha 0.0 = Pure Sparse\n",
        "        \"\"\"\n",
        "        if self.bm25 is None:\n",
        "            print(\"‚ö†Ô∏è BM25 Index not built! Running build_bm25_index() now...\")\n",
        "            self.build_bm25_index()\n",
        "\n",
        "        # 1. Dense Search (Chroma)\n",
        "        query_embedding = self.embedding_model.encode([query_text]).tolist()\n",
        "        chroma_res = self.collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=top_k * 2 # Fetch more candidates for re-ranking\n",
        "        )\n",
        "\n",
        "        # Normalize Dense Scores (Distance -> Similarity)\n",
        "        # Chroma returns distance by default for L2. If using cosine, it returns 1 - cos.\n",
        "        # Assuming default L2: Lower is better. We need to invert it.\n",
        "        # Ideally, we map scores to [0,1]. Here is a simplified normalization.\n",
        "        dense_hits = {}\n",
        "        if chroma_res['ids'] and chroma_res['ids'][0]:\n",
        "            ids = chroma_res['ids'][0]\n",
        "            distances = chroma_res['distances'][0]\n",
        "            max_dist = max(distances) + 1e-6\n",
        "            for doc_id, dist in zip(ids, distances):\n",
        "                score = 1 - (dist / max_dist) # Simple inversion normalization\n",
        "                dense_hits[doc_id] = score\n",
        "\n",
        "        # 2. Sparse Search (BM25)\n",
        "        tokenized_query = self._tokenize(query_text)\n",
        "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
        "\n",
        "        # Get Top-N BM25 IDs\n",
        "        # We need to map integer indices back to string IDs\n",
        "        top_n_indices = np.argsort(bm25_scores)[::-1][:top_k * 2]\n",
        "\n",
        "        sparse_hits = {}\n",
        "        max_bm25 = max(bm25_scores) + 1e-6\n",
        "        for idx in top_n_indices:\n",
        "            doc_id = self.bm25_ids[idx]\n",
        "            score = bm25_scores[idx] / max_bm25 # Normalize to [0,1]\n",
        "            sparse_hits[doc_id] = score\n",
        "\n",
        "        # 3. Fuse Scores\n",
        "        combined_scores = {}\n",
        "        all_ids = set(dense_hits.keys()) | set(sparse_hits.keys())\n",
        "\n",
        "        for doc_id in all_ids:\n",
        "            d_score = dense_hits.get(doc_id, 0.0)\n",
        "            s_score = sparse_hits.get(doc_id, 0.0)\n",
        "            combined_scores[doc_id] = (d_score * alpha) + (s_score * (1 - alpha))\n",
        "\n",
        "        # 4. Sort and Return\n",
        "        sorted_ids = sorted(combined_scores, key=combined_scores.get, reverse=True)[:top_k]\n",
        "\n",
        "        results = []\n",
        "        for doc_id in sorted_ids:\n",
        "            # Fetch content (we have it in memory or chroma)\n",
        "            content = self.chunk_registry.get(doc_id, \"Error: Content missing\")\n",
        "            results.append({\n",
        "                \"id\": doc_id,\n",
        "                \"score\": combined_scores[doc_id],\n",
        "                \"content\": content,\n",
        "                \"dense_score\": dense_hits.get(doc_id, 0.0),\n",
        "                \"sparse_score\": sparse_hits.get(doc_id, 0.0)\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# --- 3. The Ingestion Loop ---\n",
        "\n",
        "def ingest_directory(directory_path: str, rag_system):\n",
        "    print(f\"üìÇ Scanning '{directory_path}'...\")\n",
        "    md_files = glob.glob(os.path.join(directory_path, \"**\", \"*.md\"), recursive=True)\n",
        "\n",
        "    if not md_files:\n",
        "        print(\"‚ö†Ô∏è No markdown files found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üöÄ Processing {len(md_files)} files...\")\n",
        "\n",
        "    for file_path in tqdm(md_files):\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "\n",
        "            if not content.strip(): continue\n",
        "\n",
        "            rel_path = os.path.relpath(file_path, directory_path)\n",
        "\n",
        "            # This single call handles both Dense & Sparse prep\n",
        "            rag_system.embed_and_store(content, rel_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in {file_path}: {e}\")\n",
        "\n",
        "    # IMPORTANT: Finalize the sparse index\n",
        "    rag_system.build_bm25_index()\n",
        "\n",
        "# --- 4. Execution ---\n",
        "\n"
      ],
      "metadata": {
        "id": "qemASEoz_-WG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize\n",
        "rag = HybridRAG()\n",
        "\n",
        "# 2. Ingest your directory (Uses the function from previous step)\n",
        "# Make sure to run the 'ingest_directory' function definition from the previous turn!\n",
        "ingest_directory('/content/extracted_md', rag)\n",
        "\n",
        "# 3. IMPORTANT: Build the Index!\n",
        "# BM25 requires the full corpus to calculate IDF weights.\n",
        "rag.build_bm25_index()\n",
        "\n",
        "# 4. Test Hybrid Search\n",
        "query = \"retry logic for payment gateways\"\n",
        "results = rag.hybrid_query(query, top_k=3, alpha=0.5)\n",
        "\n",
        "print(f\"\\nüîç Hybrid Results for: '{query}'\\n\")\n",
        "for res in results:\n",
        "    print(f\"üìÑ [{res['id']}] Score: {res['score']:.4f} (Dense: {res['dense_score']:.2f}, Sparse: {res['sparse_score']:.2f})\")\n",
        "    print(f\"   {res['content'][:100]}...\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "CntZS-r3J8uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install & Load Spacy (for Entity Extraction)\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"‚úÖ Spacy loaded for entity extraction.\")"
      ],
      "metadata": {
        "id": "PAIk2-twDwNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. The Active Retrieval Control System\n",
        "\n",
        "class RetrievalJudge:\n",
        "    def __init__(self,\n",
        "                 min_max_score: float = 0.45,  # Threshold 1: At least one good hit\n",
        "                 min_avg_score: float = 0.2,   # Threshold 2: Top-k shouldn't be garbage\n",
        "                 min_entity_overlap: int = 2): # Threshold 3: Must contain specific terms\n",
        "        self.min_max_score = min_max_score\n",
        "        self.min_avg_score = min_avg_score\n",
        "        self.min_entity_overlap = min_entity_overlap\n",
        "\n",
        "    def _extract_entities(self, text: str) -> set:\n",
        "        \"\"\"Extracts Nouns and Proper Nouns as 'Entities'.\"\"\"\n",
        "        doc = nlp(text.lower())\n",
        "        # Keep Nouns (NOIJ) and Proper Nouns (PROPN) longer than 2 chars\n",
        "        return {token.text for token in doc if token.pos_ in ['NOUN', 'PROPN'] and len(token.text) > 2}\n",
        "\n",
        "    def evaluate(self, query: str, results: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Judges the quality of the retrieved set.\n",
        "        Returns: { 'pass': bool, 'reason': str, 'metrics': dict }\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return {\"pass\": False, \"reason\": \"No results found\", \"metrics\": {}}\n",
        "\n",
        "        # 1. Calculate Score Metrics\n",
        "        scores = [r['score'] for r in results]\n",
        "        max_score = max(scores)\n",
        "        avg_score = np.mean(scores)\n",
        "\n",
        "        # 2. Calculate Entity Overlap\n",
        "        query_entities = self._extract_entities(query)\n",
        "\n",
        "        # We check if the Top 1 chunk contains the query entities\n",
        "        # (Strictness: Can be relaxed to check Top 3)\n",
        "        top_chunk_text = results[0]['content'].lower()\n",
        "        overlap_count = sum(1 for entity in query_entities if entity in top_chunk_text)\n",
        "\n",
        "        metrics = {\n",
        "            \"max_score\": round(max_score, 3),\n",
        "            \"avg_score\": round(avg_score, 3),\n",
        "            \"overlap_count\": overlap_count,\n",
        "            \"query_entities\": list(query_entities)\n",
        "        }\n",
        "\n",
        "        # 3. Decision Logic (The \"Gates\")\n",
        "        if max_score < self.min_max_score:\n",
        "            return {\"pass\": False, \"reason\": f\"Max similarity too low ({max_score} < {self.min_max_score})\", \"metrics\": metrics}\n",
        "\n",
        "        if avg_score < self.min_avg_score:\n",
        "            return {\"pass\": False, \"reason\": f\"Avg quality too low ({avg_score} < {self.min_avg_score})\", \"metrics\": metrics}\n",
        "\n",
        "        # Only enforce overlap if the query actually HAD entities\n",
        "        if len(query_entities) > 0 and overlap_count < self.min_entity_overlap:\n",
        "             # Fallback: If score is REALLY high, maybe ignore overlap? (Optional logic)\n",
        "             if max_score < 0.75:\n",
        "                return {\"pass\": False, \"reason\": f\"Insufficient entity overlap ({overlap_count} < {self.min_entity_overlap})\", \"metrics\": metrics}\n",
        "\n",
        "        return {\"pass\": True, \"reason\": \"All checks passed\", \"metrics\": metrics}\n",
        "\n",
        "\n",
        "class QueryRewriter:\n",
        "    def __init__(self, llm_client=None):\n",
        "        self.llm_client = llm_client # Pass your Gemini/OpenAI client here\n",
        "\n",
        "    def generate_rewrite(self, original_query: str, fail_reason: str, history: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Uses an LLM to rewrite the query based on the failure reason.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- PROMPT TEMPLATE ---\n",
        "        system_prompt = f\"\"\"\n",
        "        You are a Query Engineering AI. The user's original query failed retrieval.\n",
        "\n",
        "        Original Query: \"{original_query}\"\n",
        "        Failure Reason: \"{fail_reason}\"\n",
        "        Previous Attempts: {history}\n",
        "\n",
        "        Task: Generate a BETTER query to satisfy the retrieval system.\n",
        "        - If failure was \"Max similarity too low\", try using broader keywords or synonyms.\n",
        "        - If failure was \"Entity overlap\", include the specific missing technical terms explicitly.\n",
        "        - If failure was \"Avg quality too low\", make the query more specific/targeted.\n",
        "\n",
        "        Output ONLY the rewritten query text. Do not add explanations.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- LLM CALL PLACEHOLDER ---\n",
        "        # Replace this block with: response = client.generate(system_prompt)\n",
        "\n",
        "        print(f\"\\nü§ñ [LLM] Rewriting query... (Reason: {fail_reason})\")\n",
        "\n",
        "        # --- SIMULATED LOGIC FOR DEMO (Remove this when using real LLM) ---\n",
        "        # This simulates an LLM improving the query\n",
        "        if \"entity\" in fail_reason.lower():\n",
        "            return original_query + \" specifications definitions\" # simplistic expansion\n",
        "        elif \"similarity\" in fail_reason.lower():\n",
        "            return \"detailed \" + original_query # simplistic expansion\n",
        "        else:\n",
        "            return original_query + \" context\"\n",
        "        # -------------------------------------------------------------\n",
        "\n",
        "\n",
        "class ActiveRetrievalLoop:\n",
        "    def __init__(self, rag_system, judge: RetrievalJudge, rewriter: QueryRewriter):\n",
        "        self.rag = rag_system\n",
        "        self.judge = judge\n",
        "        self.rewriter = rewriter\n",
        "        self.max_retries = 3\n",
        "\n",
        "    def run(self, user_query: str):\n",
        "        current_query = user_query\n",
        "        query_history = []\n",
        "\n",
        "        print(f\"üöÄ Starting Active Retrieval for: '{user_query}'\")\n",
        "\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            print(f\"\\n--- Attempt {attempt + 1} ---\")\n",
        "            print(f\"üîç Executing Query: '{current_query}'\")\n",
        "\n",
        "            # 1. Retrieve\n",
        "            # Note: We assume 'rag.hybrid_query' exists from previous step\n",
        "            results = self.rag.hybrid_query(current_query, top_k=5)\n",
        "\n",
        "            # 2. Judge\n",
        "            evaluation = self.judge.evaluate(current_query, results)\n",
        "            metrics = evaluation['metrics']\n",
        "\n",
        "            print(f\"üìä Metrics: MaxSim={metrics.get('max_score')} | Avg={metrics.get('avg_score')} | Overlap={metrics.get('overlap_count')}\")\n",
        "\n",
        "            # 3. Check Success\n",
        "            if evaluation['pass']:\n",
        "                print(\"‚úÖ Retrieval QUALITY MET! Proceeding to answer generation.\")\n",
        "                return {\n",
        "                    \"final_query\": current_query,\n",
        "                    \"results\": results,\n",
        "                    \"attempts\": attempt + 1,\n",
        "                    \"status\": \"success\"\n",
        "                }\n",
        "\n",
        "            print(f\"‚ùå Quality Check Failed: {evaluation['reason']}\")\n",
        "\n",
        "            # 4. Terminate if max retries reached\n",
        "            if attempt == self.max_retries:\n",
        "                print(\"üõë Max retries reached. Returning best effort results.\")\n",
        "                return {\n",
        "                    \"final_query\": current_query,\n",
        "                    \"results\": results,\n",
        "                    \"attempts\": attempt + 1,\n",
        "                    \"status\": \"failed_max_retries\"\n",
        "                }\n",
        "\n",
        "            # 5. Rewrite\n",
        "            query_history.append(current_query)\n",
        "            current_query = self.rewriter.generate_rewrite(user_query, evaluation['reason'], query_history)\n",
        "\n",
        "            # Check for oscillating/duplicate queries\n",
        "            if current_query in query_history:\n",
        "                print(\"‚ö†Ô∏è Loop Detected: Rewriter produced a duplicate query. Stopping.\")\n",
        "                break\n",
        "\n",
        "        return {\"status\": \"failed_loop_error\", \"results\": results}"
      ],
      "metadata": {
        "id": "yGm8vd2qEppY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Google GenAI SDK\n",
        "!pip install -q -U google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# --- ENTER YOUR API KEY HERE ---\n",
        "# You can get one at https://aistudio.google.com/app/apikey\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"Your GEMINI API KEY\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "VeHxMB7_Gbxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Gemini Query Rewriter\n",
        "\n",
        "class GeminiQueryRewriter:\n",
        "    def __init__(self, model_name: str = \"gemini-2.5-flash\"):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    def generate_rewrite(self, original_query: str, fail_reason: str, history: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Calls Gemini to rewrite the query based on the failure diagnostic.\n",
        "        \"\"\"\n",
        "\n",
        "        # Construct a prompt that treats the LLM as a control system component\n",
        "        prompt = f\"\"\"\n",
        "        You are a Query Refinement Engine for a RAG system. The dataset is about Terms and Conditions of the Indian Railways.\n",
        "        The system failed to retrieve relevant documents for the user's query.\n",
        "\n",
        "        --- DIAGNOSTICS ---\n",
        "        Original Query: \"{original_query}\"\n",
        "        Failure Reason: {fail_reason}\n",
        "        Prior Failed Attempts: {history}\n",
        "\n",
        "        --- INSTRUCTIONS ---\n",
        "        Based on the failure reason, generate ONE single improved search query.\n",
        "\n",
        "        1. If reason is \"Max similarity too low\":\n",
        "           - The query is likely too specific or uses wrong terminology.\n",
        "           - Strategy: Generalize terms, remove noisy adjectives, or use domain synonyms.\n",
        "\n",
        "        2. If reason is \"Insufficient entity overlap\":\n",
        "           - The query lacks specific nouns found in the corpus.\n",
        "           - Strategy: Add specific technical nouns, identifiers, or standard terminology related to the topic.\n",
        "\n",
        "        3. If reason is \"Avg quality too low\":\n",
        "           - The query is ambiguous and fetching mixed results.\n",
        "           - Strategy: Add context or constraints to narrow the scope.\n",
        "\n",
        "        --- OUTPUT ---\n",
        "        Output ONLY the rewritten query string. Do not output explanations or quotes.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            # Call Gemini\n",
        "            response = self.model.generate_content(prompt)\n",
        "            rewritten_query = response.text.strip().replace('\"', '').replace(\"'\", \"\")\n",
        "\n",
        "            print(f\"ü§ñ [Gemini] Reason: '{fail_reason}' | Rewrite: '{rewritten_query}'\")\n",
        "            return rewritten_query\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Gemini Error: {e}\")\n",
        "            # Fallback strategy if API fails: just append a generic term\n",
        "            return f\"{original_query} specification\"\n",
        "\n",
        "# --- Update the Loop Setup ---\n",
        "\n",
        "# 1. Initialize Components\n",
        "# We use the same Judge and RAG pipeline from before\n",
        "judge = RetrievalJudge(min_max_score=0.45, min_entity_overlap=1)\n",
        "\n",
        "# 2. Initialize the NEW Gemini Rewriter\n",
        "gemini_rewriter = GeminiQueryRewriter()\n",
        "\n",
        "# 3. Create the Loop\n",
        "active_rag = ActiveRetrievalLoop(rag, judge, gemini_rewriter)\n",
        "\n",
        "# 4. Run it\n",
        "# Example: A query that might fail initially due to lack of specific terms\n",
        "print(\"--- Starting Test Run ---\")\n",
        "final_result = active_rag.run(\"When are the charts prepared?\")"
      ],
      "metadata": {
        "id": "NqBDAaNWGYEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Final Answer Generator (Grounded)\n",
        "\n",
        "class AnswerGenerator:\n",
        "    def __init__(self, model_name: str = \"gemini-2.5-flash\"):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    def _format_context(self, results: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Formats retrieved chunks into a clean context block for the LLM.\n",
        "        \"\"\"\n",
        "        context_str = \"\"\n",
        "        for i, res in enumerate(results):\n",
        "            # Extract metadata safely\n",
        "            source = res.get('source_file', res.get('metadata', {}).get('source', 'Unknown Source'))\n",
        "            content = res.get('content', '')\n",
        "\n",
        "            context_str += f\"\"\"\n",
        "            --- CONTEXT BLOCK {i+1} ---\n",
        "            Source: {source}\n",
        "            Content:\n",
        "            {content}\n",
        "            ---------------------------\n",
        "            \"\"\"\n",
        "        return context_str\n",
        "\n",
        "    def generate_answer(self, query: str, results: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Generates the final answer using strictly the provided context.\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return \"I could not find enough information in the documents to answer your question.\"\n",
        "\n",
        "        context_text = self._format_context(results)\n",
        "\n",
        "        # --- SYSTEM PROMPT ---\n",
        "        system_prompt = f\"\"\"\n",
        "        You are a Technical Documentation Assistant.\n",
        "        Answer the user's question using ONLY the context blocks provided below.\n",
        "\n",
        "        --- STRICT RULES ---\n",
        "        1. **Grounding:** Do not use outside knowledge. If the answer is not in the context, say \"I cannot find this information in the provided documents.\"\n",
        "        2. **Citation:** When you state a fact, mention the source file name (e.g., [Source: specs/v1.md]).\n",
        "        3. **Tone:** Professional, concise, and technical.\n",
        "        4. **Formatting:** Use Markdown lists or code blocks where appropriate.\n",
        "\n",
        "        --- RETRIEVED CONTEXT ---\n",
        "        {context_text}\n",
        "\n",
        "        --- USER QUESTION ---\n",
        "        {query}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(system_prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# --- 4. Tying It All Together (The Full Pipeline) ---\n",
        "\n",
        "# 1. Initialize Generator\n",
        "answer_gen = AnswerGenerator()\n",
        "\n",
        "# 2. Run the Active Loop (from previous steps)\n",
        "# This handles the Query -> Judge -> Rewrite -> Retrieve cycle\n",
        "user_query = \"Quota for ticket booking\"\n",
        "loop_result = active_rag.run(user_query)\n",
        "\n",
        "print(f\"\\n‚úÖ Loop Finished with status: {loop_result['status']}\")\n",
        "\n",
        "# 3. Generate Final Answer (if successful)\n",
        "if loop_result['results']:\n",
        "    print(\"\\nüìù Generating Final Answer...\\n\")\n",
        "    final_answer = answer_gen.generate_answer(\n",
        "        query=user_query,\n",
        "        results=loop_result['results']\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(final_answer)\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"‚ùå Failed to retrieve valid documents. Cannot answer.\")"
      ],
      "metadata": {
        "id": "5odTDkOCGejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDekXrfmMRRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}